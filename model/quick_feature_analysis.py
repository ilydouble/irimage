#!/usr/bin/env python3
"""
å¿«é€Ÿç‰¹å¾é‡è¦æ€§åˆ†æè„šæœ¬
ä¸“é—¨ç”¨äºå¿«é€Ÿæµ‹è¯•ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
"""

import sys
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Tuple, Any
import time

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from run_interpretability_analysis import find_latest_model, find_dataset_dir, detect_model_asymmetry_mode
from interpretability_analysis import ThermalInterpretabilityAnalyzer

class QuickFeatureAnalyzer:
    """å¿«é€Ÿç‰¹å¾é‡è¦æ€§åˆ†æå™¨"""
    
    def __init__(self, output_dir: str = "dataset/datasets/quick_analysis"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.analysis_results = []
        self.layer_statistics = {}
        
    def run_quick_analysis(self, model_path: str = None, dataset_dir: str = None, max_images: int = 20):
        """è¿è¡Œå¿«é€Ÿåˆ†æï¼Œé™åˆ¶å›¾åƒæ•°é‡"""
        print("=== å¿«é€Ÿç‰¹å¾é‡è¦æ€§åˆ†æ ===\n")
        
        start_time = time.time()
        
        # 1. è‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹å’Œæ•°æ®é›†
        if model_path is None:
            model_path = "model/contrastive_thermal_classifier_results/run_20250827_180427__last/best_classifier.pth"
        if dataset_dir is None:
            dataset_dir = find_dataset_dir()
            
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"æ•°æ®é›†è·¯å¾„: {dataset_dir}")
        
        # 2. æ£€æµ‹æ¨¡å‹æ¨¡å¼
        use_asymmetry = detect_model_asymmetry_mode(model_path)
        print(f"æ¨¡å‹æ¨¡å¼: {'ä¸å¯¹ç§°åˆ†æ (6é€šé“)' if use_asymmetry else 'æ ‡å‡†æ¨¡å¼ (3é€šé“)'}")
        
        # 3. åˆ›å»ºåˆ†æå™¨
        analyzer = ThermalInterpretabilityAnalyzer(
            model_path=model_path,
            use_asymmetry_analysis=use_asymmetry
        )
        
        # 4. æ”¶é›†å°‘é‡å›¾åƒè¿›è¡Œå¿«é€Ÿæµ‹è¯•
        image_paths = self._collect_sample_images(dataset_dir, max_images)
        print(f"é€‰æ‹© {len(image_paths)} å¼ å›¾åƒè¿›è¡Œå¿«é€Ÿåˆ†æ")
        
        # 5. å¿«é€Ÿåˆ†æ
        print("\nå¼€å§‹å¿«é€Ÿåˆ†æ...")
        results = []
        for i, (image_path, true_label) in enumerate(image_paths):
            print(f"è¿›åº¦: {i+1}/{len(image_paths)} - {Path(image_path).name}")
            try:
                result = analyzer.analyze_single_image(str(image_path))
                result['true_label'] = true_label
                result['image_name'] = Path(image_path).name
                results.append(result)
            except Exception as e:
                print(f"  âŒ åˆ†æå¤±è´¥: {e}")
                continue
        
        self.analysis_results = results
        print(f"\næˆåŠŸåˆ†æ {len(results)} å¼ å›¾åƒ")
        
        # 6. å¿«é€Ÿç»Ÿè®¡åˆ†æ
        print("\nå¼€å§‹ç»Ÿè®¡åˆ†æ...")
        self._quick_layer_analysis()
        
        # 7. ç”Ÿæˆç®€åŒ–æŠ¥å‘Š
        print("\nç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        self._generate_quick_report()
        
        total_time = time.time() - start_time
        print(f"\n=== å¿«é€Ÿåˆ†æå®Œæˆ ===")
        print(f"æ€»è€—æ—¶: {total_time:.1f}ç§’")
        print(f"ç»“æœä¿å­˜åˆ°: {self.output_dir}")
        
    def _collect_sample_images(self, dataset_dir: str, max_images: int) -> List[Tuple[str, int]]:
        """æ”¶é›†æ ·æœ¬å›¾åƒ - éšæœºé‡‡æ ·ï¼Œä¸æŒ‰ç›®å½•åå‘"""
        import random

        image_paths = []
        dataset_path = Path(dataset_dir)

        # æ”¶é›†æ‰€æœ‰å›¾åƒæ–‡ä»¶ï¼Œä¸åŒºåˆ†ç›®å½•
        all_image_files = list(dataset_path.glob("**/*.jpg"))
        print(f"æ‰¾åˆ°æ€»è®¡ {len(all_image_files)} å¼ å›¾åƒ")

        # éšæœºæ‰“ä¹±é¡ºåºï¼Œé¿å…ç›®å½•åå‘
        random.shuffle(all_image_files)

        # é™åˆ¶æ•°é‡
        selected_images = all_image_files[:max_images]

        # ä¸ºæ¯å¼ å›¾åƒåˆ†é…æ ‡ç­¾
        icas_count = 0
        non_icas_count = 0
        unknown_count = 0

        for img_path in selected_images:
            # å°è¯•ä»è·¯å¾„æ¨æ–­æ ‡ç­¾
            path_str = str(img_path).lower()
            if "icas" in path_str and "non_icas" not in path_str:
                label = 1  # ICAS
                icas_count += 1
            elif "non_icas" in path_str or "non-icas" in path_str:
                label = 0  # Non-ICAS
                non_icas_count += 1
            else:
                # æ— æ³•ä»è·¯å¾„åˆ¤æ–­ï¼Œè®¾ä¸ºæœªçŸ¥
                label = -1  # æœªçŸ¥æ ‡ç­¾
                unknown_count += 1

            image_paths.append((str(img_path), label))

        print(f"  éšæœºé€‰æ‹©çš„æ ·æœ¬åˆ†å¸ƒ:")
        print(f"  ICAS: {icas_count} å¼ ")
        print(f"  Non-ICAS: {non_icas_count} å¼ ")
        print(f"  æœªçŸ¥: {unknown_count} å¼ ")

        return image_paths
    
    def _quick_layer_analysis(self):
        """å¿«é€Ÿå±‚é‡è¦æ€§åˆ†æ"""
        print("  ğŸ“Š åˆ†æå±‚é‡è¦æ€§...")
        
        layer_stats = {}
        
        for result in self.analysis_results:
            for layer_name, layer_result in result['gradcam_results'].items():
                if layer_name not in layer_stats:
                    layer_stats[layer_name] = {
                        'max_activations': [],
                        'mean_activations': [],
                        'activation_areas': [],
                        'confidence_scores': []
                    }
                
                stats_data = layer_result['feature_statistics']
                layer_stats[layer_name]['max_activations'].append(stats_data['max_activation'])
                layer_stats[layer_name]['mean_activations'].append(stats_data['mean_activation'])
                layer_stats[layer_name]['activation_areas'].append(stats_data['activation_area'])
                layer_stats[layer_name]['confidence_scores'].append(result['confidence'])
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        for layer_name, stats in layer_stats.items():
            layer_summary = {
                'layer_name': layer_name,
                'avg_max_activation': np.mean(stats['max_activations']),
                'avg_mean_activation': np.mean(stats['mean_activations']),
                'avg_activation_area': np.mean(stats['activation_areas']),
                'activation_consistency': 1 - np.std(stats['mean_activations']) / (np.mean(stats['mean_activations']) + 1e-8),
                'avg_confidence': np.mean(stats['confidence_scores']),
                'sample_count': len(stats['max_activations'])
            }
            
            self.layer_statistics[layer_name] = layer_summary
    
    def _generate_quick_report(self):
        """ç”Ÿæˆå¿«é€Ÿåˆ†ææŠ¥å‘Š"""
        # 1. ä¿å­˜è¯¦ç»†ç»“æœ
        results_data = {
            'analysis_results': self.analysis_results,
            'layer_statistics': self.layer_statistics,
            'analysis_summary': self._generate_summary()
        }
        
        with open(self.output_dir / 'quick_analysis_results.json', 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        # 2. ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        report_path = self.output_dir / 'quick_analysis_report.txt'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=== å¿«é€Ÿç‰¹å¾é‡è¦æ€§åˆ†ææŠ¥å‘Š ===\n\n")
            
            # åŸºæœ¬ä¿¡æ¯
            f.write(f"åˆ†ææ ·æœ¬æ•°: {len(self.analysis_results)}\n")
            f.write(f"åˆ†æå±‚æ•°: {len(self.layer_statistics)}\n\n")
            
            # å±‚é‡è¦æ€§æ’å
            if self.layer_statistics:
                # æŒ‰å¹³å‡æœ€å¤§æ¿€æ´»æ’åº
                sorted_layers = sorted(
                    self.layer_statistics.items(), 
                    key=lambda x: x[1]['avg_max_activation'], 
                    reverse=True
                )
                
                f.write("=== å±‚é‡è¦æ€§æ’å (æŒ‰å¹³å‡æœ€å¤§æ¿€æ´») ===\n")
                for i, (layer, stats) in enumerate(sorted_layers, 1):
                    f.write(f"{i}. {layer}: {stats['avg_max_activation']:.4f}\n")
                f.write("\n")
                
                # è¯¦ç»†ç»Ÿè®¡
                f.write("=== è¯¦ç»†å±‚ç»Ÿè®¡ ===\n")
                for layer, stats in sorted_layers:
                    f.write(f"\n{layer}:\n")
                    f.write(f"  å¹³å‡æœ€å¤§æ¿€æ´»: {stats['avg_max_activation']:.4f}\n")
                    f.write(f"  å¹³å‡æ¿€æ´»å€¼: {stats['avg_mean_activation']:.4f}\n")
                    f.write(f"  å¹³å‡æ¿€æ´»åŒºåŸŸ: {stats['avg_activation_area']:.4f}\n")
                    f.write(f"  æ¿€æ´»ä¸€è‡´æ€§: {stats['activation_consistency']:.4f}\n")
                    f.write(f"  å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence']:.4f}\n")
                    f.write(f"  æ ·æœ¬æ•°: {stats['sample_count']}\n")
            
            f.write(f"\næŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\n")
        
        # 3. ç”Ÿæˆç®€å•å¯è§†åŒ–
        self._generate_quick_visualization()
    
    def _generate_quick_visualization(self):
        """ç”Ÿæˆå¿«é€Ÿå¯è§†åŒ–"""
        if not self.layer_statistics:
            return
        
        try:
            layers = list(self.layer_statistics.keys())
            max_activations = [self.layer_statistics[layer]['avg_max_activation'] for layer in layers]
            
            plt.figure(figsize=(12, 6))
            bars = plt.bar(range(len(layers)), max_activations, alpha=0.7)
            plt.xlabel('ç½‘ç»œå±‚')
            plt.ylabel('å¹³å‡æœ€å¤§æ¿€æ´»å€¼')
            plt.title('å„å±‚ç‰¹å¾é‡è¦æ€§å¿«é€Ÿåˆ†æ')
            plt.xticks(range(len(layers)), [l.split('.')[-1] for l in layers], rotation=45)
            
            # æ ‡æ³¨æ•°å€¼
            for i, bar in enumerate(bars):
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height,
                        f'{max_activations[i]:.3f}', ha='center', va='bottom')
            
            plt.tight_layout()
            plt.savefig(self.output_dir / 'quick_layer_importance.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            print("  âœ… å¿«é€Ÿå¯è§†åŒ–å·²ç”Ÿæˆ")
            
        except Exception as e:
            print(f"  âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        if not self.layer_statistics:
            return {}
        
        # æ‰¾å‡ºæœ€é‡è¦çš„å±‚
        best_layer = max(
            self.layer_statistics.items(), 
            key=lambda x: x[1]['avg_max_activation']
        )
        
        return {
            'most_important_layer': best_layer[0],
            'highest_activation': best_layer[1]['avg_max_activation'],
            'total_samples': len(self.analysis_results),
            'layers_analyzed': len(self.layer_statistics)
        }

def main():
    """ä¸»å‡½æ•°"""
    print("å¿«é€Ÿç‰¹å¾é‡è¦æ€§åˆ†æå·¥å…·\n")
    
    # è·å–ç”¨æˆ·è¾“å…¥
    try:
        max_images = int(input("è¯·è¾“å…¥è¦åˆ†æçš„å›¾åƒæ•°é‡ (é»˜è®¤20): ") or "20")
    except:
        max_images = 20
    
    analyzer = QuickFeatureAnalyzer()
    
    try:
        analyzer.run_quick_analysis(max_images=max_images)
        print("\nğŸ‰ å¿«é€Ÿåˆ†æå®Œæˆ!")
        print(f"ğŸ“Š æŸ¥çœ‹ç»“æœ: {analyzer.output_dir}")
        print("ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: quick_analysis_report.txt")
        print("ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨: quick_layer_importance.png")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
