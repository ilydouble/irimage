#!/usr/bin/env python3
"""
å…¨é¢çš„ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡åˆ†æè„šæœ¬
åˆ†ææ‰€æœ‰å›¾åƒçš„Grad-CAMç»“æœï¼Œç»Ÿè®¡ç‰¹å¾å±‚é‡è¦æ€§å’Œä½ç½®æ¨¡å¼
"""

import sys
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Tuple, Any
import cv2
from scipy import stats
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from run_interpretability_analysis import find_latest_model, find_dataset_dir, detect_model_asymmetry_mode
from interpretability_analysis import ThermalInterpretabilityAnalyzer

class FeatureImportanceAnalyzer:
    """ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡åˆ†æå™¨"""
    
    def __init__(self, output_dir: str = "dataset/datasets/feature_importance_analysis"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / "statistics").mkdir(exist_ok=True)
        (self.output_dir / "visualizations").mkdir(exist_ok=True)
        (self.output_dir / "heatmap_clusters").mkdir(exist_ok=True)
        (self.output_dir / "layer_comparisons").mkdir(exist_ok=True)
        
        # åˆ†æç»“æœå­˜å‚¨
        self.analysis_results = []
        self.layer_statistics = {}
        self.position_patterns = {}
        self.class_differences = {}
        
    def run_comprehensive_analysis(self, model_path: str = None, dataset_dir: str = None):
        """è¿è¡Œå…¨é¢åˆ†æ"""
        print("=== å¼€å§‹å…¨é¢ç‰¹å¾é‡è¦æ€§åˆ†æ ===\n")
        
        # 1. è‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹å’Œæ•°æ®é›†
        if model_path is None:
            model_path =  "model/contrastive_thermal_classifier_results/run_20250827_180427__last/best_classifier.pth"#find_latest_model()
        if dataset_dir is None:
            dataset_dir = find_dataset_dir()
            
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"æ•°æ®é›†è·¯å¾„: {dataset_dir}")
        
        # 2. æ£€æµ‹æ¨¡å‹æ¨¡å¼
        use_asymmetry = detect_model_asymmetry_mode(model_path)
        print(f"æ¨¡å‹æ¨¡å¼: {'ä¸å¯¹ç§°åˆ†æ (6é€šé“)' if use_asymmetry else 'æ ‡å‡†æ¨¡å¼ (3é€šé“)'}")
        
        # 3. åˆ›å»ºåˆ†æå™¨
        analyzer = ThermalInterpretabilityAnalyzer(
            model_path=model_path,
            use_asymmetry_analysis=use_asymmetry
        )
        
        # 4. æ”¶é›†æ‰€æœ‰å›¾åƒ
        image_paths = self._collect_all_images(dataset_dir)
        print(f"æ‰¾åˆ° {len(image_paths)} å¼ å›¾åƒè¿›è¡Œåˆ†æ")
        
        # 5. æ‰¹é‡åˆ†ææ‰€æœ‰å›¾åƒ - æ·»åŠ é™åˆ¶å’Œè¿›åº¦æ˜¾ç¤º
        print("\nå¼€å§‹æ‰¹é‡åˆ†æ...")

        # é™åˆ¶åˆ†æçš„å›¾åƒæ•°é‡ï¼Œé¿å…è¿‡é•¿æ—¶é—´
        max_images = min(200, len(image_paths))  # æœ€å¤šåˆ†æ200å¼ å›¾åƒ
        if len(image_paths) > max_images:
            print(f"å›¾åƒæ•°é‡è¿‡å¤š({len(image_paths)})ï¼Œé™åˆ¶ä¸ºå‰{max_images}å¼ ")
            image_paths = image_paths[:max_images]

        results = []
        import time
        start_time = time.time()

        for i, (image_path, true_label) in enumerate(image_paths):
            # æ˜¾ç¤ºè¿›åº¦
            if i % 10 == 0 or i == len(image_paths) - 1:
                elapsed = time.time() - start_time
                avg_time = elapsed / (i + 1) if i > 0 else 0
                remaining = avg_time * (len(image_paths) - i - 1)
                print(f"è¿›åº¦: {i+1}/{len(image_paths)} ({(i+1)/len(image_paths)*100:.1f}%) - "
                      f"å·²ç”¨æ—¶: {elapsed:.1f}s, é¢„è®¡å‰©ä½™: {remaining:.1f}s")

            try:
                result = analyzer.analyze_single_image(str(image_path))
                result['true_label'] = true_label
                result['image_name'] = Path(image_path).name
                results.append(result)
            except Exception as e:
                print(f"  âŒ åˆ†æå¤±è´¥ {Path(image_path).name}: {e}")
                continue

            # æ—¶é—´é™åˆ¶ï¼šå¦‚æœå•å¼ å›¾åƒåˆ†ææ—¶é—´è¿‡é•¿ï¼Œè·³è¿‡åç»­
            if i > 0 and (time.time() - start_time) / (i + 1) > 30:  # å¹³å‡æ¯å¼ è¶…è¿‡30ç§’
                print(f"âš ï¸  åˆ†æé€Ÿåº¦è¿‡æ…¢ï¼Œåœæ­¢åç»­åˆ†æ")
                break
        
        self.analysis_results = results
        print(f"\næˆåŠŸåˆ†æ {len(results)} å¼ å›¾åƒ")
        
        # 6. ç»Ÿè®¡åˆ†æ
        print("\nå¼€å§‹ç»Ÿè®¡åˆ†æ...")
        self._analyze_layer_importance()
        self._analyze_position_patterns()
        self._analyze_class_differences()
        self._analyze_prediction_accuracy()
        
        # 7. ç”Ÿæˆå¯è§†åŒ–
        print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        self._generate_visualizations()
        
        # 8. ä¿å­˜ç»“æœ
        print("\nä¿å­˜åˆ†æç»“æœ...")
        self._save_comprehensive_results()
        
        print(f"\n=== åˆ†æå®Œæˆ ===")
        print(f"ç»“æœä¿å­˜åˆ°: {self.output_dir}")
        
    def _collect_all_images(self, dataset_dir: str) -> List[Tuple[str, int]]:
        """æ”¶é›†æ‰€æœ‰å›¾åƒåŠå…¶æ ‡ç­¾ - éšæœºé‡‡æ ·ï¼Œä¸æŒ‰ç›®å½•åå‘"""
        import random

        image_paths = []
        dataset_path = Path(dataset_dir)

        # æ”¶é›†æ‰€æœ‰å›¾åƒæ–‡ä»¶ï¼Œä¸åŒºåˆ†ç›®å½•
        all_image_files = []

        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰jpgæ–‡ä»¶
        for img_path in dataset_path.glob("**/*.jpg"):
            all_image_files.append(img_path)

        print(f"æ‰¾åˆ°æ€»è®¡ {len(all_image_files)} å¼ å›¾åƒ")

        # éšæœºæ‰“ä¹±é¡ºåºï¼Œé¿å…ç›®å½•åå‘
        random.shuffle(all_image_files)

        # ä¸ºæ¯å¼ å›¾åƒåˆ†é…æ ‡ç­¾
        for img_path in all_image_files:
            # å°è¯•ä»è·¯å¾„æ¨æ–­æ ‡ç­¾
            path_str = str(img_path).lower()
            if "icas" in path_str and "non_icas" not in path_str:
                label = 1  # ICAS
            elif "non_icas" in path_str or "non-icas" in path_str:
                label = 0  # Non-ICAS
            else:
                # æ— æ³•ä»è·¯å¾„åˆ¤æ–­ï¼Œè®¾ä¸ºæœªçŸ¥
                label = -1  # æœªçŸ¥æ ‡ç­¾

            image_paths.append((str(img_path), label))

        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
        icas_count = sum(1 for _, label in image_paths if label == 1)
        non_icas_count = sum(1 for _, label in image_paths if label == 0)
        unknown_count = sum(1 for _, label in image_paths if label == -1)

        print(f"æ ‡ç­¾åˆ†å¸ƒ: ICAS={icas_count}, Non-ICAS={non_icas_count}, æœªçŸ¥={unknown_count}")

        return image_paths
    
    def _analyze_layer_importance(self):
        """åˆ†æå„å±‚é‡è¦æ€§"""
        print("  ğŸ“Š åˆ†æå±‚é‡è¦æ€§...")
        
        layer_stats = {}
        
        for result in self.analysis_results:
            for layer_name, layer_result in result['gradcam_results'].items():
                if layer_name not in layer_stats:
                    layer_stats[layer_name] = {
                        'max_activations': [],
                        'mean_activations': [],
                        'activation_areas': [],
                        'prediction_correct': [],
                        'confidence_scores': []
                    }
                
                stats_data = layer_result['feature_statistics']
                layer_stats[layer_name]['max_activations'].append(stats_data['max_activation'])
                layer_stats[layer_name]['mean_activations'].append(stats_data['mean_activation'])
                layer_stats[layer_name]['activation_areas'].append(stats_data['activation_area'])
                
                # é¢„æµ‹å‡†ç¡®æ€§
                pred_correct = (result['predicted_class'] == result['true_label']) if result['true_label'] != -1 else None
                layer_stats[layer_name]['prediction_correct'].append(pred_correct)
                layer_stats[layer_name]['confidence_scores'].append(result['confidence'])
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        for layer_name, stats in layer_stats.items():
            layer_summary = {
                'layer_name': layer_name,
                'avg_max_activation': np.mean(stats['max_activations']),
                'std_max_activation': np.std(stats['max_activations']),
                'avg_mean_activation': np.mean(stats['mean_activations']),
                'avg_activation_area': np.mean(stats['activation_areas']),
                'activation_consistency': 1 - np.std(stats['mean_activations']) / (np.mean(stats['mean_activations']) + 1e-8),
                'sample_count': len(stats['max_activations'])
            }
            
            # è®¡ç®—ä¸é¢„æµ‹å‡†ç¡®æ€§çš„ç›¸å…³æ€§
            valid_predictions = [p for p in stats['prediction_correct'] if p is not None]
            if valid_predictions:
                valid_confidences = [c for c, p in zip(stats['confidence_scores'], stats['prediction_correct']) if p is not None]
                layer_summary['accuracy_correlation'] = np.corrcoef(valid_predictions, valid_confidences)[0, 1] if len(valid_predictions) > 1 else 0
            else:
                layer_summary['accuracy_correlation'] = 0
            
            self.layer_statistics[layer_name] = layer_summary
    
    def _analyze_position_patterns(self):
        """åˆ†æä½ç½®æ¨¡å¼ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘è®¡ç®—æ—¶é—´"""
        print("  ğŸ“ åˆ†æä½ç½®æ¨¡å¼...")

        # é™åˆ¶åˆ†æçš„æ ·æœ¬æ•°é‡ï¼Œé¿å…è¿‡é•¿çš„è®¡ç®—æ—¶é—´
        max_samples_per_layer = 500  # æ¯å±‚æœ€å¤šåˆ†æ100ä¸ªæ ·æœ¬

        for layer_name in self.layer_statistics.keys():
            centers_of_mass = []

            # æ”¶é›†ä½ç½®æ•°æ®ï¼Œä½†é™åˆ¶æ•°é‡
            sample_count = 0
            for result in self.analysis_results:
                if sample_count >= max_samples_per_layer:
                    break

                if layer_name in result['gradcam_results']:
                    stats_data = result['gradcam_results'][layer_name]['feature_statistics']
                    centers_of_mass.append(stats_data['center_of_mass'])
                    sample_count += 1

            if centers_of_mass:
                centers_array = np.array(centers_of_mass)

                # è®¡ç®—ä½ç½®ç»Ÿè®¡
                position_stats = {
                    'mean_center': np.mean(centers_array, axis=0).tolist(),
                    'std_center': np.std(centers_array, axis=0).tolist(),
                    'center_spread': np.mean(np.std(centers_array, axis=0)),
                    'sample_count': len(centers_of_mass),
                    'total_available': len([r for r in self.analysis_results if layer_name in r['gradcam_results']])
                }

                # ç®€åŒ–çš„èšç±»åˆ†æï¼Œåªåœ¨æ ·æœ¬æ•°é€‚ä¸­æ—¶è¿›è¡Œ
                if 5 <= len(centers_array) <= 50:  # åªåœ¨åˆç†èŒƒå›´å†…è¿›è¡Œèšç±»
                    try:
                        n_clusters = min(3, len(centers_array) // 3)  # å‡å°‘èšç±»æ•°
                        if n_clusters >= 2:
                            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=5)  # å‡å°‘åˆå§‹åŒ–æ¬¡æ•°
                            clusters = kmeans.fit_predict(centers_array)
                            position_stats['cluster_centers'] = kmeans.cluster_centers_.tolist()
                            position_stats['cluster_labels'] = clusters.tolist()
                        else:
                            position_stats['cluster_centers'] = []
                            position_stats['cluster_labels'] = []
                    except Exception as e:
                        print(f"    èšç±»åˆ†æå¤±è´¥ {layer_name}: {e}")
                        position_stats['cluster_centers'] = []
                        position_stats['cluster_labels'] = []
                else:
                    # æ ·æœ¬æ•°ä¸åˆé€‚ï¼Œè·³è¿‡èšç±»
                    position_stats['cluster_centers'] = []
                    position_stats['cluster_labels'] = []
                    if len(centers_array) > 50:
                        print(f"    {layer_name}: æ ·æœ¬æ•°è¿‡å¤š({len(centers_array)})ï¼Œè·³è¿‡èšç±»åˆ†æ")

                self.position_patterns[layer_name] = position_stats
                print(f"    {layer_name}: åˆ†æäº† {len(centers_of_mass)} ä¸ªæ ·æœ¬")
            else:
                print(f"    {layer_name}: æ— å¯ç”¨æ ·æœ¬")
    
    def _analyze_class_differences(self):
        """åˆ†æç±»åˆ«å·®å¼‚"""
        print("  ğŸ” åˆ†æç±»åˆ«å·®å¼‚...")
        
        icas_results = [r for r in self.analysis_results if r['true_label'] == 1]
        non_icas_results = [r for r in self.analysis_results if r['true_label'] == 0]
        
        print(f"    ICASæ ·æœ¬: {len(icas_results)}")
        print(f"    Non-ICASæ ·æœ¬: {len(non_icas_results)}")
        
        for layer_name in self.layer_statistics.keys():
            icas_stats = []
            non_icas_stats = []
            
            for result in icas_results:
                if layer_name in result['gradcam_results']:
                    icas_stats.append(result['gradcam_results'][layer_name]['feature_statistics'])
            
            for result in non_icas_results:
                if layer_name in result['gradcam_results']:
                    non_icas_stats.append(result['gradcam_results'][layer_name]['feature_statistics'])
            
            if icas_stats and non_icas_stats:
                # è®¡ç®—å„æŒ‡æ ‡çš„ç±»åˆ«å·®å¼‚
                icas_max_act = [s['max_activation'] for s in icas_stats]
                non_icas_max_act = [s['max_activation'] for s in non_icas_stats]
                
                icas_mean_act = [s['mean_activation'] for s in icas_stats]
                non_icas_mean_act = [s['mean_activation'] for s in non_icas_stats]
                
                icas_area = [s['activation_area'] for s in icas_stats]
                non_icas_area = [s['activation_area'] for s in non_icas_stats]
                
                # ç»Ÿè®¡æ£€éªŒ
                try:
                    max_act_pvalue = stats.ttest_ind(icas_max_act, non_icas_max_act)[1]
                    mean_act_pvalue = stats.ttest_ind(icas_mean_act, non_icas_mean_act)[1]
                    area_pvalue = stats.ttest_ind(icas_area, non_icas_area)[1]
                except:
                    max_act_pvalue = mean_act_pvalue = area_pvalue = 1.0
                
                class_diff = {
                    'icas_samples': len(icas_stats),
                    'non_icas_samples': len(non_icas_stats),
                    'max_activation_diff': np.mean(icas_max_act) - np.mean(non_icas_max_act),
                    'mean_activation_diff': np.mean(icas_mean_act) - np.mean(non_icas_mean_act),
                    'activation_area_diff': np.mean(icas_area) - np.mean(non_icas_area),
                    'max_activation_pvalue': max_act_pvalue,
                    'mean_activation_pvalue': mean_act_pvalue,
                    'activation_area_pvalue': area_pvalue,
                    'significant_differences': []
                }
                
                # æ ‡è®°æ˜¾è‘—å·®å¼‚
                if max_act_pvalue < 0.05:
                    class_diff['significant_differences'].append('max_activation')
                if mean_act_pvalue < 0.05:
                    class_diff['significant_differences'].append('mean_activation')
                if area_pvalue < 0.05:
                    class_diff['significant_differences'].append('activation_area')
                
                self.class_differences[layer_name] = class_diff
    
    def _analyze_prediction_accuracy(self):
        """åˆ†æé¢„æµ‹å‡†ç¡®æ€§"""
        print("  ğŸ¯ åˆ†æé¢„æµ‹å‡†ç¡®æ€§...")
        
        # æ•´ä½“å‡†ç¡®æ€§
        valid_predictions = [r for r in self.analysis_results if r['true_label'] != -1]
        if valid_predictions:
            correct_predictions = sum(1 for r in valid_predictions if r['predicted_class'] == r['true_label'])
            overall_accuracy = correct_predictions / len(valid_predictions)
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            icas_predictions = [r for r in valid_predictions if r['true_label'] == 1]
            non_icas_predictions = [r for r in valid_predictions if r['true_label'] == 0]
            
            icas_correct = sum(1 for r in icas_predictions if r['predicted_class'] == r['true_label'])
            non_icas_correct = sum(1 for r in non_icas_predictions if r['predicted_class'] == r['true_label'])
            
            self.prediction_accuracy = {
                'overall_accuracy': overall_accuracy,
                'total_samples': len(valid_predictions),
                'correct_predictions': correct_predictions,
                'icas_accuracy': icas_correct / len(icas_predictions) if icas_predictions else 0,
                'non_icas_accuracy': non_icas_correct / len(non_icas_predictions) if non_icas_predictions else 0,
                'icas_samples': len(icas_predictions),
                'non_icas_samples': len(non_icas_predictions)
            }
            
            print(f"    æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f}")
            print(f"    ICASå‡†ç¡®ç‡: {self.prediction_accuracy['icas_accuracy']:.4f}")
            print(f"    Non-ICASå‡†ç¡®ç‡: {self.prediction_accuracy['non_icas_accuracy']:.4f}")
    
    def _generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        print("  ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. å±‚é‡è¦æ€§å¯¹æ¯”
        self._plot_layer_importance()
        
        # 2. ä½ç½®æ¨¡å¼åˆ†æ
        self._plot_position_patterns()
        
        # 3. ç±»åˆ«å·®å¼‚åˆ†æ
        self._plot_class_differences()
        
        # 4. é¢„æµ‹å‡†ç¡®æ€§åˆ†æ
        self._plot_prediction_analysis()
        
        # 5. ç»¼åˆçƒ­åŠ›å›¾
        self._plot_comprehensive_heatmap()
    
    def _plot_layer_importance(self):
        """ç»˜åˆ¶å±‚é‡è¦æ€§å›¾è¡¨"""
        if not self.layer_statistics:
            return
            
        layers = list(self.layer_statistics.keys())
        metrics = ['avg_max_activation', 'avg_mean_activation', 'avg_activation_area', 'activation_consistency']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('å„å±‚ç‰¹å¾é‡è¦æ€§åˆ†æ', fontsize=16, fontweight='bold')
        
        for i, metric in enumerate(metrics):
            ax = axes[i//2, i%2]
            values = [self.layer_statistics[layer][metric] for layer in layers]
            
            bars = ax.bar(range(len(layers)), values, alpha=0.7)
            ax.set_xlabel('ç½‘ç»œå±‚')
            ax.set_ylabel(metric.replace('_', ' ').title())
            ax.set_title(f'{metric.replace("_", " ").title()} å¯¹æ¯”')
            ax.set_xticks(range(len(layers)))
            ax.set_xticklabels([l.split('.')[-1] for l in layers], rotation=45)
            
            # æ ‡æ³¨æ•°å€¼
            for j, bar in enumerate(bars):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{values[j]:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'visualizations' / 'layer_importance.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_position_patterns(self):
        """ç»˜åˆ¶ä½ç½®æ¨¡å¼å›¾è¡¨ - ä¼˜åŒ–ç‰ˆæœ¬"""
        if not self.position_patterns:
            print("    è·³è¿‡ä½ç½®æ¨¡å¼å›¾è¡¨ï¼šæ— æ•°æ®")
            return

        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('æ¿€æ´»ä½ç½®æ¨¡å¼åˆ†æ', fontsize=16, fontweight='bold')

            layers = list(self.position_patterns.keys())[:4]  # æœ€å¤šæ˜¾ç¤º4å±‚

            for i, layer in enumerate(layers):
                if i >= 4:
                    break

                ax = axes[i//2, i%2]
                pattern = self.position_patterns[layer]

                # ç»˜åˆ¶ä¸­å¿ƒä½ç½®åˆ†å¸ƒ
                mean_center = pattern['mean_center']
                std_center = pattern['std_center']

                # åˆ›å»ºæ•£ç‚¹å›¾æ˜¾ç¤ºä½ç½®åˆ†å¸ƒ
                ax.scatter(mean_center[1], mean_center[0], s=200, c='red', marker='x', label='å¹³å‡ä¸­å¿ƒ')

                # ç®€åŒ–æ¤­åœ†ç»˜åˆ¶ï¼Œé¿å…å¤æ‚è®¡ç®—
                try:
                    from matplotlib.patches import Ellipse
                    ellipse = Ellipse(xy=(mean_center[1], mean_center[0]),
                                    width=std_center[1]*2, height=std_center[0]*2,
                                    alpha=0.3, facecolor='blue', label='æ ‡å‡†å·®èŒƒå›´')
                    ax.add_patch(ellipse)
                except:
                    # å¦‚æœæ¤­åœ†ç»˜åˆ¶å¤±è´¥ï¼Œè·³è¿‡
                    pass

                ax.set_xlabel('Xåæ ‡')
                ax.set_ylabel('Yåæ ‡')
                ax.set_title(f'{layer.split(".")[-1]} æ¿€æ´»ä¸­å¿ƒåˆ†å¸ƒ\n(æ ·æœ¬æ•°: {pattern["sample_count"]})')
                ax.legend()
                ax.grid(True, alpha=0.3)

            # éšè—ç©ºçš„å­å›¾
            for i in range(len(layers), 4):
                axes[i//2, i%2].axis('off')

            plt.tight_layout()
            plt.savefig(self.output_dir / 'visualizations' / 'position_patterns.png', dpi=300, bbox_inches='tight')
            plt.close()
            print("    âœ… ä½ç½®æ¨¡å¼å›¾è¡¨å·²ç”Ÿæˆ")

        except Exception as e:
            print(f"    âŒ ä½ç½®æ¨¡å¼å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
            plt.close()
    
    def _plot_class_differences(self):
        """ç»˜åˆ¶ç±»åˆ«å·®å¼‚å›¾è¡¨"""
        if not self.class_differences:
            return
            
        layers = list(self.class_differences.keys())
        metrics = ['max_activation_diff', 'mean_activation_diff', 'activation_area_diff']
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        x = np.arange(len(layers))
        width = 0.25
        
        for i, metric in enumerate(metrics):
            values = [self.class_differences[layer][metric] for layer in layers]
            pvalues = [self.class_differences[layer][metric.replace('_diff', '_pvalue')] for layer in layers]
            
            bars = ax.bar(x + i*width, values, width, label=metric.replace('_diff', '').replace('_', ' ').title())
            
            # æ ‡è®°æ˜¾è‘—æ€§
            for j, (bar, pval) in enumerate(zip(bars, pvalues)):
                if pval < 0.05:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           '*', ha='center', va='bottom', fontweight='bold', fontsize=14)
        
        ax.set_xlabel('ç½‘ç»œå±‚')
        ax.set_ylabel('ICAS - Non-ICAS å·®å¼‚')
        ax.set_title('å„å±‚ç‰¹å¾çš„ç±»åˆ«å·®å¼‚åˆ†æ (* p<0.05)')
        ax.set_xticks(x + width)
        ax.set_xticklabels([l.split('.')[-1] for l in layers], rotation=45)
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'visualizations' / 'class_differences.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_prediction_analysis(self):
        """ç»˜åˆ¶é¢„æµ‹åˆ†æå›¾è¡¨"""
        if not hasattr(self, 'prediction_accuracy'):
            return
            
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # 1. æ•´ä½“å‡†ç¡®ç‡
        acc_data = self.prediction_accuracy
        categories = ['Overall', 'ICAS', 'Non-ICAS']
        accuracies = [acc_data['overall_accuracy'], acc_data['icas_accuracy'], acc_data['non_icas_accuracy']]
        
        bars1 = ax1.bar(categories, accuracies, color=['skyblue', 'lightcoral', 'lightgreen'])
        ax1.set_ylabel('å‡†ç¡®ç‡')
        ax1.set_title('é¢„æµ‹å‡†ç¡®ç‡åˆ†æ')
        ax1.set_ylim(0, 1)
        
        for bar, acc in zip(bars1, accuracies):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{acc:.3f}', ha='center', va='bottom')
        
        # 2. æ ·æœ¬åˆ†å¸ƒ
        sample_counts = [acc_data['icas_samples'], acc_data['non_icas_samples']]
        labels = ['ICAS', 'Non-ICAS']
        
        ax2.pie(sample_counts, labels=labels, autopct='%1.1f%%', startangle=90)
        ax2.set_title('æ ·æœ¬åˆ†å¸ƒ')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'visualizations' / 'prediction_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_comprehensive_heatmap(self):
        """ç»˜åˆ¶ç»¼åˆçƒ­åŠ›å›¾"""
        if not self.layer_statistics or not self.class_differences:
            return
            
        # åˆ›å»ºç»¼åˆæŒ‡æ ‡çŸ©é˜µ
        layers = list(self.layer_statistics.keys())
        metrics = ['avg_max_activation', 'avg_mean_activation', 'activation_consistency', 'max_activation_diff']
        
        data_matrix = []
        for layer in layers:
            row = []
            layer_stats = self.layer_statistics[layer]
            row.extend([
                layer_stats['avg_max_activation'],
                layer_stats['avg_mean_activation'], 
                layer_stats['activation_consistency']
            ])
            
            if layer in self.class_differences:
                row.append(abs(self.class_differences[layer]['max_activation_diff']))
            else:
                row.append(0)
            
            data_matrix.append(row)
        
        # æ ‡å‡†åŒ–æ•°æ®
        data_matrix = np.array(data_matrix)
        data_normalized = (data_matrix - data_matrix.min(axis=0)) / (data_matrix.max(axis=0) - data_matrix.min(axis=0) + 1e-8)
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        plt.figure(figsize=(10, 8))
        sns.heatmap(data_normalized.T, 
                   xticklabels=[l.split('.')[-1] for l in layers],
                   yticklabels=[m.replace('_', ' ').title() for m in metrics],
                   annot=True, fmt='.3f', cmap='YlOrRd')
        
        plt.title('å„å±‚ç‰¹å¾é‡è¦æ€§ç»¼åˆçƒ­åŠ›å›¾', fontsize=14, fontweight='bold')
        plt.xlabel('ç½‘ç»œå±‚')
        plt.ylabel('ç‰¹å¾æŒ‡æ ‡')
        plt.tight_layout()
        plt.savefig(self.output_dir / 'visualizations' / 'comprehensive_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _save_comprehensive_results(self):
        """ä¿å­˜ç»¼åˆåˆ†æç»“æœ"""
        # ä¿å­˜è¯¦ç»†ç»Ÿè®¡ç»“æœ
        comprehensive_results = {
            'layer_statistics': self.layer_statistics,
            'position_patterns': self.position_patterns,
            'class_differences': self.class_differences,
            'prediction_accuracy': getattr(self, 'prediction_accuracy', {}),
            'analysis_summary': self._generate_analysis_summary()
        }
        
        with open(self.output_dir / 'comprehensive_analysis_results.json', 'w', encoding='utf-8') as f:
            json.dump(comprehensive_results, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        self._generate_text_report()
    
    def _generate_analysis_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        if not self.layer_statistics:
            return {}
            
        # æ‰¾å‡ºæœ€é‡è¦çš„å±‚
        layer_importance_scores = {}
        for layer, stats in self.layer_statistics.items():
            # ç»¼åˆè¯„åˆ†ï¼šæ¿€æ´»å¼ºåº¦ + ä¸€è‡´æ€§ + ç±»åˆ«åŒºåˆ†åº¦
            score = (stats['avg_max_activation'] * 0.3 + 
                    stats['activation_consistency'] * 0.3)
            
            if layer in self.class_differences:
                class_diff = self.class_differences[layer]
                significant_count = len(class_diff['significant_differences'])
                score += significant_count * 0.4 / 3  # æœ€å¤š3ä¸ªæ˜¾è‘—å·®å¼‚
            
            layer_importance_scores[layer] = score
        
        # æ’åºæ‰¾å‡ºæœ€é‡è¦çš„å±‚
        sorted_layers = sorted(layer_importance_scores.items(), key=lambda x: x[1], reverse=True)
        
        return {
            'most_important_layer': sorted_layers[0][0] if sorted_layers else None,
            'layer_importance_ranking': [layer for layer, score in sorted_layers],
            'layer_importance_scores': layer_importance_scores,
            'total_samples_analyzed': len(self.analysis_results),
            'layers_analyzed': len(self.layer_statistics)
        }
    
    def _generate_text_report(self):
        """ç”Ÿæˆæ–‡æœ¬åˆ†ææŠ¥å‘Š"""
        report_path = self.output_dir / 'comprehensive_analysis_report.txt'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=== å…¨é¢ç‰¹å¾é‡è¦æ€§åˆ†ææŠ¥å‘Š ===\n\n")
            
            # åŸºæœ¬ä¿¡æ¯
            f.write(f"åˆ†ææ ·æœ¬æ€»æ•°: {len(self.analysis_results)}\n")
            f.write(f"åˆ†æå±‚æ•°: {len(self.layer_statistics)}\n\n")
            
            # é¢„æµ‹å‡†ç¡®æ€§
            if hasattr(self, 'prediction_accuracy'):
                acc = self.prediction_accuracy
                f.write("=== é¢„æµ‹å‡†ç¡®æ€§ ===\n")
                f.write(f"æ•´ä½“å‡†ç¡®ç‡: {acc['overall_accuracy']:.4f}\n")
                f.write(f"ICASå‡†ç¡®ç‡: {acc['icas_accuracy']:.4f} ({acc['icas_samples']} æ ·æœ¬)\n")
                f.write(f"Non-ICASå‡†ç¡®ç‡: {acc['non_icas_accuracy']:.4f} ({acc['non_icas_samples']} æ ·æœ¬)\n\n")
            
            # å±‚é‡è¦æ€§æ’å
            if 'analysis_summary' in self.__dict__ or hasattr(self, 'layer_statistics'):
                summary = self._generate_analysis_summary()
                f.write("=== å±‚é‡è¦æ€§æ’å ===\n")
                for i, layer in enumerate(summary['layer_importance_ranking'], 1):
                    score = summary['layer_importance_scores'][layer]
                    f.write(f"{i}. {layer}: {score:.4f}\n")
                f.write(f"\næœ€é‡è¦çš„å±‚: {summary['most_important_layer']}\n\n")
            
            # æ˜¾è‘—ç±»åˆ«å·®å¼‚
            f.write("=== æ˜¾è‘—ç±»åˆ«å·®å¼‚ ===\n")
            for layer, diff in self.class_differences.items():
                if diff['significant_differences']:
                    f.write(f"{layer}:\n")
                    for sig_diff in diff['significant_differences']:
                        pvalue = diff[f"{sig_diff}_pvalue"]
                        f.write(f"  - {sig_diff}: p={pvalue:.4f}\n")
            
            f.write(f"\næŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\n")
            f.write(f"ç»“æœä¿å­˜ç›®å½•: {self.output_dir}\n")

def main():
    """ä¸»å‡½æ•°"""
    print("å…¨é¢ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡åˆ†æå·¥å…·\n")
    
    analyzer = FeatureImportanceAnalyzer()
    
    try:
        analyzer.run_comprehensive_analysis()
        print("\nğŸ‰ å…¨é¢åˆ†æå®Œæˆ!")
        print(f"ğŸ“Š æŸ¥çœ‹ç»“æœ: {analyzer.output_dir}")
        print("ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨: visualizations/ ç›®å½•")
        print("ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: comprehensive_analysis_report.txt")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

# ä½¿ç”¨ç¤ºä¾‹
"""
# 1. ç›´æ¥è¿è¡Œå…¨é¢åˆ†æï¼ˆè‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹å’Œæ•°æ®é›†ï¼‰
python model/comprehensive_feature_analysis.py

# 2. æŒ‡å®šæ¨¡å‹å’Œæ•°æ®é›†è·¯å¾„
from model.comprehensive_feature_analysis import FeatureImportanceAnalyzer

analyzer = FeatureImportanceAnalyzer()
analyzer.run_comprehensive_analysis(
    model_path="path/to/your/model.pth",
    dataset_dir="path/to/your/dataset"
)
"""
